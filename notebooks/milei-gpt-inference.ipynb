{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f60772e-3472-45ae-844b-3b442e77fc89",
   "metadata": {},
   "source": [
    "# Milei-GPT Inference\n",
    "- www.machinelearnear.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60040647-bb0e-4ceb-8812-55c38a791474",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
    "!pip install --upgrade gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21992c1e-661a-4673-93d9-af6262644571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = 'YOUR_CACHE_DIR'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad8d979-2400-48ea-ade2-bdbbb37425ba",
   "metadata": {},
   "source": [
    "## Ejecutar en la Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894e2878-93b9-42eb-8125-6552a8b3f4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from unsloth import FastLanguageModel\n",
    "# import torch\n",
    "# max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "# dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "# load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# # load model\n",
    "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#     model_name = \"machinelearnear/llama-3-8b-milei-gpt\",\n",
    "#     # model_name = \"machinelearnear/Phi-3-medium-4k-instruct-milei-gpt\",\n",
    "#     # model_name = \"machinelearnear/mistral-7b-instruct-v0.3-bnb-4bit-milei-gpt\",\n",
    "#     max_seq_length = max_seq_length,\n",
    "#     dtype = dtype,\n",
    "#     load_in_4bit = load_in_4bit,\n",
    "#     # token = \"\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "# )\n",
    "\n",
    "# FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f747b646-e874-4efc-917a-7828b4528524",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # function to read yaml file and get system message\n",
    "# def read_yaml(config_path):\n",
    "#     import yaml\n",
    "#     with open(config_path, 'r') as file:\n",
    "#         return yaml.safe_load(file)\n",
    "    \n",
    "# config = read_yaml(\"../scripts/unsloth_finetuning_config.yaml\")\n",
    "# system_message = config.get('system_message', 'You are a nice bot!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbceb5a7-6d61-4ede-9f83-299a91240021",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# messages = [\n",
    "#     {\n",
    "#         \"content\": system_message,\n",
    "#         \"role\": \"system\",\n",
    "#     },\n",
    "#     {\n",
    "#         \"content\": \"Che te comerias un asado con unos politicos?\",\n",
    "#         \"role\": \"user\"\n",
    "#     },\n",
    "# ]\n",
    "\n",
    "# inputs = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     tokenize = True,\n",
    "#     add_generation_prompt = True, # Must add for generation\n",
    "#     return_tensors = \"pt\",\n",
    "# ).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50313b31-3e04-4d06-b0f5-d3164db09119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TextStreamer\n",
    "# text_streamer = TextStreamer(tokenizer)\n",
    "# _ = model.generate(\n",
    "#     input_ids = inputs,\n",
    "#     streamer = text_streamer,\n",
    "#     max_new_tokens = 1000,\n",
    "#     use_cache = True,\n",
    "#     temperature=0.1, # creativity and randomness of the response\n",
    "#     top_p=0.9, # dynamically adjusts the number of choices for each predicted token, which helps to maintain diversity and generate more fluent and natural-sounding text\n",
    "#     top_k=50, # limits the number of choices for the next predicted word or token, which helps to speed up the generation process and can improve the quality of the generated text\n",
    "#     repetition_penalty=1.2, # reduce the likelihood of repeating prompt text or getting stuck in a loop\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa743fef-b236-48a7-9195-67fa699efd2a",
   "metadata": {},
   "source": [
    "## Ejecutar dentro de una app de Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9557e6d0-2044-4329-b751-ca66faefcf61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "from threading import Thread\n",
    "from typing import Iterator\n",
    "\n",
    "import gradio as gr\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n",
    "import time\n",
    "\n",
    "MAX_MAX_NEW_TOKENS = 2048\n",
    "DEFAULT_MAX_NEW_TOKENS = 1000\n",
    "MAX_INPUT_TOKEN_LENGTH = 4096\n",
    "DTYPE = None\n",
    "LOAD_IN_4BIT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b39b951-a737-4d87-8053-8feeb8ac0303",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import yaml\n",
    "\n",
    "CONFIG_PATH = \"../scripts/unsloth_finetuning_config.yaml\"\n",
    "if not os.path.exists(CONFIG_PATH):\n",
    "\tresponse = requests.get(\"https://raw.githubusercontent.com/machinelearnear/milei-gpt/main/scripts/unsloth_finetuning_config.yaml\")\n",
    "\tconfig = yaml.safe_load(response.content)\n",
    "else:\n",
    "\tconfig = yaml.safe_load(CONFIG_PATH)\n",
    "\n",
    "system_prompt = config.get('system_message', 'You are a nice bot!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b46db00-daa3-40c2-a217-bf48d0b9e654",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PLACEHOLDER = \"\"\"\n",
    "<div style=\"padding: 50px; text-align: center; display: flex; flex-direction: column; align-items: center;\">\n",
    "   <img src=\"https://avatars.githubusercontent.com/u/78419164?v=4\" style=\"width: 80%; max-width: 550px; height: auto; opacity: 0.80;  \"> \n",
    "   <h1 style=\"font-size: 36px; margin-bottom: 15px; opacity: 0.85; font-family: 'Source Sans Pro', sans-serif;\">Llama-3-8B-Instruct-Milei-GPT</h1>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "custom_css = \"\"\"\n",
    "@import url('https://fonts.googleapis.com/css2?family=Source+Sans+Pro:wght@300;400;700&display=swap');\n",
    "\n",
    "body, .gradio-container, .gr-button, .gr-input, .gr-slider, .gr-dropdown, .gr-markdown {\n",
    "    font-family: 'Source Sans Pro', sans-serif !important;\n",
    "    margin: 0;\n",
    "    padding: 0;\n",
    "    box-sizing: border-box;\n",
    "    font-size: 18px;\n",
    "}\n",
    "._button {\n",
    "    font-size: 20px;\n",
    "    padding: 12px 24px;\n",
    "}\n",
    "pre, code {\n",
    "    direction: ltr !important;\n",
    "    unicode-bidi: plaintext !important;\n",
    "    font-size: 16px;\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2daa7c-42d4-4904-b407-795393092965",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def execution_time_calculator(start_time, log=True):\n",
    "    delta = time.time() - start_time\n",
    "    if log:\n",
    "        print(\"--- %s seconds ---\" % (delta))\n",
    "    return delta\n",
    "\n",
    "def token_per_second_calculator(tokens_count, time_delta):\n",
    "    return tokens_count/time_delta\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    DESCRIPTION = \"\\n<p>Running on CPU ü•∂ This demo does not work on CPU.</p>\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Load model and tokenizer\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"machinelearnear/llama-3-8b-milei-gpt\",\n",
    "        max_seq_length=MAX_MAX_NEW_TOKENS,\n",
    "        dtype=DTYPE,\n",
    "        load_in_4bit=LOAD_IN_4BIT,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    \n",
    "generation_speed = 0\n",
    "\n",
    "def get_generation_speed():\n",
    "    global generation_speed\n",
    "    return generation_speed\n",
    "\n",
    "def generate(\n",
    "    message: str,\n",
    "    chat_history: list[tuple[str, str]],\n",
    "    max_new_tokens: int = 1000,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9,\n",
    "    top_k: int = 30,\n",
    "    repetition_penalty: float = 1.2,\n",
    "    do_sample: bool =True,\n",
    ") -> Iterator[str]:\n",
    "    global generation_speed\n",
    "    global system_prompt\n",
    "\n",
    "    conversation = []\n",
    "    if system_prompt:\n",
    "        conversation.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    for user, assistant in chat_history:\n",
    "        conversation.extend([{\"role\": \"user\", \"content\": user}, {\"role\": \"assistant\", \"content\": assistant}])\n",
    "    conversation.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(conversation, return_tensors=\"pt\")\n",
    "    if input_ids.shape[1] > MAX_INPUT_TOKEN_LENGTH:\n",
    "        input_ids = input_ids[:, -MAX_INPUT_TOKEN_LENGTH:]\n",
    "        gr.Warning(f\"Trimmed input from conversation as it was longer than {MAX_INPUT_TOKEN_LENGTH} tokens.\")\n",
    "    input_ids = input_ids.to(model.device)\n",
    "\n",
    "    streamer = TextIteratorStreamer(tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens=True)\n",
    "    generate_kwargs = dict(\n",
    "        {\"input_ids\": input_ids},\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        temperature=temperature,\n",
    "        num_beams=1, # streamer only works with num_beams=1\n",
    "        repetition_penalty=repetition_penalty,\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
    "    t.start()\n",
    "\n",
    "    outputs = []\n",
    "    sum_tokens = 0\n",
    "    for text in streamer:\n",
    "        num_tokens = len(tokenizer.tokenize(text))\n",
    "        sum_tokens += num_tokens\n",
    "        \n",
    "        outputs.append(text)\n",
    "        yield \"\".join(outputs)\n",
    "\n",
    "    time_delta = execution_time_calculator(start_time, log=False)\n",
    "\n",
    "    generation_speed = token_per_second_calculator(sum_tokens, time_delta)\n",
    "\n",
    "    print(f\"generation_speed: {generation_speed} tk/s\")\n",
    "\n",
    "\n",
    "chatbot = gr.Chatbot(placeholder=PLACEHOLDER, scale=1, show_copy_button=True, height=\"68%\", rtl=False,  elem_classes=[\"chatbot\"])\n",
    "chat_input = gr.Textbox(show_label=True, rtl=False, placeholder=\"Input\", show_copy_button=True, scale=5)\n",
    "submit_btn = gr.Button(variant=\"primary\", value=\"Submit\", size=\"sm\", scale=1, elem_classes=[\"_button\"])\n",
    "\n",
    "\n",
    "chat_interface = gr.ChatInterface(\n",
    "    fn=generate,\n",
    "    additional_inputs_accordion=gr.Accordion(label=\"Configuration\", open=False),\n",
    "    additional_inputs=[\n",
    "        gr.Slider(\n",
    "            label=\"Maximum number of tokens\",\n",
    "            minimum=1,\n",
    "            maximum=MAX_MAX_NEW_TOKENS,\n",
    "            step=1,\n",
    "            value=DEFAULT_MAX_NEW_TOKENS,\n",
    "        ),\n",
    "        gr.Slider(\n",
    "            label=\"Temperature\",\n",
    "            minimum=0.01,\n",
    "            maximum=4.0,\n",
    "            step=0.01,\n",
    "            value=0.1,\n",
    "        ),\n",
    "        gr.Slider(\n",
    "            label=\"Top-p\",\n",
    "            minimum=0.05,\n",
    "            maximum=1.0,\n",
    "            step=0.01,\n",
    "            value=0.9,\n",
    "        ),\n",
    "        gr.Slider(\n",
    "            label=\"Top-k\",\n",
    "            minimum=1,\n",
    "            maximum=1000,\n",
    "            step=1,\n",
    "            value=30,\n",
    "        ),\n",
    "        gr.Slider(\n",
    "            label=\"Repetition penalty\",\n",
    "            minimum=1.0,\n",
    "            maximum=2.0,\n",
    "            step=0.05,\n",
    "            value=1.20,\n",
    "        ),\n",
    "        gr.Dropdown(\n",
    "            label=\"Sampling\",\n",
    "            choices=[False, True],\n",
    "            value=True)\n",
    "    ],\n",
    "    stop_btn=\"Stop\",\n",
    "    chatbot=chatbot,\n",
    "    textbox=chat_input,\n",
    "    submit_btn=submit_btn,\n",
    "    retry_btn=\"üîÑ Retry\",\n",
    "    undo_btn=\"‚Ü©Ô∏è Undo\",\n",
    "    clear_btn=\"üóëÔ∏è Clear\",\n",
    "    title=\"Llama-3-8B-Instruct-Milei-GPT Chatbot\",\n",
    "    description=\"Hecho con ‚ù§Ô∏è y üßâ por @machinelearnear\",\n",
    "    examples=[\n",
    "        [\"Te ir√≠as a comer un asado con alguien de la casta pol√≠tica?\"],\n",
    "        [\"Che y cuantos perros tenes realmente? dame los nombres\"],\n",
    "        [\"Las calles deberian ser privadas o p√∫blicas?\"],\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "with gr.Blocks(css=custom_css, fill_height=False) as demo:\n",
    "    # gr.Markdown(DESCRIPTION)\n",
    "    chat_interface.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3765f09-5883-4874-a069-2664b53ec2e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "demo.queue(max_size=20).launch(debug=True, share=True, inline=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38971b8-d70a-4569-b49c-5ddc74c4c605",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_machinelearnear-dev",
   "language": "python",
   "name": "conda_machinelearnear-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
